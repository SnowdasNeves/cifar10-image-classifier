{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 2.0 with CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and dataset init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. You can use GPU for PyTorch.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU for PyTorch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation and normalization for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "\n",
    "# Creating train/vali split\n",
    "train_size = int(0.8 * len(full_trainset))\n",
    "val_size = len(full_trainset) - train_size\n",
    "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
    "\n",
    "valset.dataset.transform = transform_test  # For validation we switch to transform_test (no augmentation)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Input: 3x32x32, Output: 32x32x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # 32x16x16\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 64x16x16\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # 64x8x8\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),# 128x8x8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # 128x4x4\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# 128x4x4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                           # 128x2x2\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# 128x2x2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                            # 128x1x1\n",
    "        )\n",
    "\n",
    "        # 2 fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 1 * 1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseCNN().to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Compute metrics\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "rec = recall_score(all_labels, all_preds, average='weighted')\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=testset.classes, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # remove normalization from transform\n",
    "    np_img = img.numpy().transpose(1, 2, 0)\n",
    "    plt.imshow(np_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "outputs = model(images)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "indices = random.sample(range(images.size(0)), 10)\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    imshow(images[idx].cpu())\n",
    "    true_label = testset.classes[labels[idx].item()]\n",
    "    pred_label = testset.classes[preds[idx].item()]\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
